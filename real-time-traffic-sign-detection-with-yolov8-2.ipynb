{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8968421,"sourceType":"datasetVersion","datasetId":4066836},{"sourceId":8978478,"sourceType":"datasetVersion","datasetId":5406346}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n  <img src=\"https://assets-global.website-files.com/6479eab6eb2ed5e597810e9e/65bf057da592b5830c09e160_LinkedIn_Product_Page_YOLOv8.webp\" alt=\"Image\" style=\"object-fit:cover;\n            width:900px;\n            height:300px;\n            border-radius: 20px;\"/>\n</div>","metadata":{"id":"Dhszo8exgUfF"}},{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n  <img src=\"https://arxiv.org/html/2304.00501v6/x3.png\" alt=\"Image\" width=\"750\" height=\"800\">\n</div>","metadata":{"id":"I-qI55uDgUfG"}},{"cell_type":"markdown","source":"<div style=\"background-color:#FAE8F8; padding: 8px; border-radius: 30px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h1 style=\"font-size:24px; text-align:center; font-family:calibri; color:#141140;\"><b>Object Detection Mastery with Pretrained YOLOv8n</b></h1>\n</div>","metadata":{"id":"RHOptIFy2CCk"}},{"cell_type":"markdown","source":"<a id=\"contents_tabel\"></a>   \n<span style=\"color:navy;font-weight:700;font-size:30px\">\nTable of Contents  \n</span>","metadata":{"id":"EyJy_OuL2CCl"}},{"cell_type":"markdown","source":"* **[1. Installing YOLOv8](#install)**\n\n* **[2. Importing Necessary Libraries](#lib)**\n\n* **[3. Setting up Weights and Biases (wandb)](#wandb)**\n\n* **[4. Dataset Exploration](#sample)**\n\n* **[5. Data Preparation](#preparation)**\n\n* **[6. Model Training](#training)**\n\n* **[7. Model Performance Evaluation](#evaluation)**\n\n* **[8. Model Inference](#inference)**\n\n* **[9. Export the model to ONNX format](#export)**\n\n","metadata":{"id":"9WWYneUJRnyN"}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:20px; background-color: #98fb98; font-family:calibri; color: #141140; border: 3px #533078 solid; text-align: justify;line-height: 1.5em; \">\n    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b> üîé YOLOv8 Overview</b></h1>In recent times, the field of computer vision has witnessed substantial progress, and the <strong>You Only Look Once (YOLO)</strong> object detection system stands out as a prominent milestone.\n    <strong>YOLOv8</strong>, the latest iteration of this robust technology which is released in January 2023, has been brought to life by Ultralytics and boasts enhanced precision along with a host of new capabilities. As the latest offering from Ultralytics, YOLOv8 stands as a cutting-edge model for object detection, instance segmentation, and image classification. It outperforms its predecessors in terms of speed and accuracy, while also providing a comprehensive framework for model training.<strong> YOLOv8 is available in five different versions, each distinguished by the number of parameters they possess : Nano (n), Small (s), Medium (m), Large (l), and Extra Large (x)</strong>.\n</div>","metadata":{"id":"QEH4PZ2QgUfI"}},{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n  <img src=\"https://miro.medium.com/v2/resize:fit:908/format:webp/1*BkJdaTpGdmVacqOO8WDPrg.png\" alt=\"Image\" width=\"400\" height=\"400\">\n</div>","metadata":{"id":"7yNMf7HzgUfJ"}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:20px; background-color: #98fb98; font-family:calibri; color: #141140; border: 3px #533078 solid; text-align: justify;line-height: 1.5em; \">\n    \n    \n<strong>YOLOv8 Nano (n)</strong> is designed specifically for running on mobile and embedded devices. It offers exceptional speed and efficiency while maintaining a compact footprint. Its streamlined architecture makes it ideal for real-time applications where computational resources are limited.\n\nNow, let‚Äôs delve into the <strong>YOLOv8 Small (s)</strong> model. This variant strikes a balance between performance and resource constraints. While it retains the speed advantages of Nano, it also provides improved accuracy. YOLOv8 Small is an excellent choice for scenarios where you need reliable object detection without sacrificing too much computational efficiency.\n    \nLet‚Äôs explore the characteristics of YOLOv8 Medium (m),YOLOv8 Large (l), and YOLOv8 Extra Large (x):\n    \n<strong>YOLOv8 Medium (m)</strong> is a versatile choice with a balanced trade-off between speed and accuracy.\n    \n<strong>YOLOv8 Large (l)</strong> provides higher accuracy at the expense of slightly reduced speed which is suitable for scenarios where precision is critical.\n    \n<strong>YOLOv8 Extra Large (x)</strong> is the largest and most accurate variant, ideal for applications demanding top-tier performance.\n    \n<strong style=\"background-color:#ececec\">YOLOv8</strong> was developed and trained using <strong style=\"background-color:#ececec\">PyTorch</strong> and then saved as files with the <strong style=\"background-color:#ececec\">.pt</strong> extension.\n    <strong>In the current project, I specifically utilized the yolov8n.pt model, which falls into the category of nano-sized models designed for object detection tasks.</strong><br>\n</div>","metadata":{"id":"94rem778RnyL"}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:20px;text-align: justify; background-color: #98fb98; font-family:calibri; color: #141140; border: 3px #533078 solid;line-height: 3rem; \">\n    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b> üéØ Project Objectives:</b></h1>\n    \n<li><strong>1. Detecting Traffic Signs:</strong>\nCreate an accurate model capable of detecting a wide range of traffic road signs in real-world images or video streams.\n    \n<li><strong>2. Classifying Signs:</strong>\nClassify detected signs into specific types, including <strong style=\"background-color:#ececec\">\"SPEED LIMIT\"</strong> and <strong style=\"background-color:#ececec\">\"STOP,\"</strong> as well as <strong style=\"background-color:#ececec\">\"traffic lights\"</strong> such as Green Light and Red Light.\n\n<li><strong>3. Model Fine-Tuning for Aerial Vehicle Detection:</strong>\nUtilize transfer learning to fine-tune YOLOv8.\n\n<li><strong>4. Inference and Generalization on Test Data:</strong>\nTest the model‚Äôs generalization on validation images, unseen test images, and a test video.\n\n<li><strong>5. Real-Time Traffic Road Sign Recognition:</strong>\nDevelop an algorithm to recognize traffic road signs in real-time using test video data.\n\n<li><strong>6. Deployment:</strong>\nPrepare the model for deployment in practical applications such as autonomous vehicles or traffic management systems by exporting the fine-tuned model in <strong style=\"background-color:#ececec\">\"ONNX\"</strong> format.\n</div>","metadata":{"id":"l4sZn9iORnyM"}},{"cell_type":"markdown","source":"### üí™ Now let‚Äôs delve into the project üí™","metadata":{"id":"qlIyODgBRnyN"}},{"cell_type":"markdown","source":"<a id=\"install\"></a>\n# <p style=\"background-image: url(https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/22606/images/61ae8d7-6831-7f5c-8b52-01d30ba74ffc_og-ultralytics.jpeg);font-family:tahoma; font-weight:bold; color:navy; font-size:100%;text-align:center; border-radius:50px 50px;padding:7px; border:solid 2px #09375b; box-shadow: 10px 10px 10px #042b4c\">1. Installing YOLOv8</p>\n‚úÖ [Tabel of Contents](#contents_tabel)","metadata":{"id":"aCYWu44EgUfL"}},{"cell_type":"code","source":"!pip install ultralytics\nfrom IPython import display\ndisplay.clear_output()\n\nimport ultralytics\nultralytics.checks()","metadata":{"id":"ft7tarIEgUfM","outputId":"8bd9e6db-7644-46f6-83b3-e19d46b66ffd","execution":{"iopub.status.busy":"2024-07-18T18:33:05.034672Z","iopub.execute_input":"2024-07-18T18:33:05.035293Z","iopub.status.idle":"2024-07-18T18:33:35.011918Z","shell.execute_reply.started":"2024-07-18T18:33:05.035256Z","shell.execute_reply":"2024-07-18T18:33:35.010921Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"lib\"></a>\n# <p style=\"background-image: url(https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/22606/images/61ae8d7-6831-7f5c-8b52-01d30ba74ffc_og-ultralytics.jpeg);font-family:tahoma; font-weight:bold; color:navy; font-size:100%;text-align:center; border-radius:50px 50px;padding:7px; border:solid 2px #09375b; box-shadow: 10px 10px 10px #042b4c\"> 2. Importing Necessary Libraries</p>\n‚úÖ [Tabel of Contents](#contents_tabel)","metadata":{"id":"FMLYct7ggUfN"}},{"cell_type":"code","source":"# Import Essential Libraries\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport cv2\nfrom ultralytics import YOLO\nfrom PIL import Image\n\n# Disable warnings in the notebook to maintain clean output cells\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"DI7Tdby3gUfN","execution":{"iopub.status.busy":"2024-07-18T18:33:35.013862Z","iopub.execute_input":"2024-07-18T18:33:35.01471Z","iopub.status.idle":"2024-07-18T18:33:35.020249Z","shell.execute_reply.started":"2024-07-18T18:33:35.014675Z","shell.execute_reply":"2024-07-18T18:33:35.019395Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"wandb\"></a>\n# <p style=\"background-image: url(https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/22606/images/61ae8d7-6831-7f5c-8b52-01d30ba74ffc_og-ultralytics.jpeg);font-family:tahoma; font-weight:bold; color:navy; font-size:100%;text-align:center; border-radius:50px 50px;padding:7px; border:solid 2px #09375b; box-shadow: 10px 10px 10px #042b4c\"> 3. Setting up Weights and Biases (wandb)</p>\n‚úÖ [Tabel of Contents](#contents_tabel)","metadata":{"id":"qd8iNvQNgUfO"}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:18px; background-color: #a7e8d9;padding: 22px; font-family:calibri; color: #141140; border: 3px #a7e8d9 solid;border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); text-align: justify;line-height: 2em; \">\n    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b> üóùÔ∏è Receiving Wandb Key and Keeping It Secure üîë</b></h1>\n    \n<li><strong>1.</strong>\nSign up for a free account on <a href=\"https://wandb.ai/\">https://wandb.ai/</a>\n    \n<li><strong>2.</strong>\nRetrieve your API key from this link:<a href=\"https://wandb.ai/authorize\">https://wandb.ai/authorize/</a> and copy it.\n\n<li><strong>3.</strong>\nImport the wandb library into your project with the following code: <code style=\"background-color:#fff\">import wandb</code>. If you are working locally, you need to first install the library using <code style=\"background-color:#fff\">!pip install wandb</code>\n\n<li><strong>4.</strong>\nTo securely store your wandb API key, import the UserSecretsClient from kaggle_secrets with: <code style=\"background-color:#fff\">from kaggle_secrets import UserSecretsClient</code>. This ensures that your API key remains hidden from other users.\n\n<li><strong>5.</strong>\nUse the <code  style=\"background-color:#fff\">UserSecretsClient()</code> to retrieve your wandb API key with the following code:\n    <code style=\"background-color:#fff\">user_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"My_Wandb\")</code>      \n\n<li><strong>6.</strong>\n    At the top of your screen, navigate to <code style=\"background-color:#fff\">Add-ons -> Secrets -> Add a new secret</code>. In the <code style=\"background-color:#fff\">Label</code> field, enter a name as for example 'My_Wandb'. In the <code>Value</code> field, paste the API key you previously copied. Ensure that \"Attach to Notebook\" is checked. After this, click save. Then press \"Done\".\n<li><strong>7.</strong>\n    log in to wandb with <code style=\"background-color:#fff\">wandb.login(secret_value_0)</code>.\n</div>","metadata":{"id":"i5Dkw-ZJ2CCs"}},{"cell_type":"code","source":"!pip install wandb\n\nfrom IPython import display\ndisplay.clear_output()\n\n!wandb --version","metadata":{"id":"5uBbGMRFRnyT","outputId":"7f31c30e-3f69-42cb-de2e-17c318b01160","execution":{"iopub.status.busy":"2024-07-18T18:33:35.021544Z","iopub.execute_input":"2024-07-18T18:33:35.022041Z","iopub.status.idle":"2024-07-18T18:33:51.309596Z","shell.execute_reply.started":"2024-07-18T18:33:35.021957Z","shell.execute_reply":"2024-07-18T18:33:51.308497Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import wandb\nimport wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"My_Wandb\")\n    wandb.login(secret_value_0)\n\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and add your W&B access token.')","metadata":{"id":"n0pt6i0hgUfO","outputId":"c8c9fff8-37d9-497b-c07f-cbadc39f70ad","execution":{"iopub.status.busy":"2024-07-18T18:33:51.312185Z","iopub.execute_input":"2024-07-18T18:33:51.312507Z","iopub.status.idle":"2024-07-18T18:33:51.84605Z","shell.execute_reply.started":"2024-07-18T18:33:51.312479Z","shell.execute_reply":"2024-07-18T18:33:51.845105Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"sample\"></a>\n# <p style=\"background-image: url(https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/22606/images/61ae8d7-6831-7f5c-8b52-01d30ba74ffc_og-ultralytics.jpeg);font-family:tahoma; font-weight:bold; color:navy; font-size:100%;text-align:center; border-radius:50px 50px;padding:7px; border:solid 2px #09375b; box-shadow: 10px 10px 10px #042b4c\"> 4. Dataset Exploration</p>\n‚úÖ [Tabel of Contents](#contents_tabel)","metadata":{"id":"cdXKHheKgUfO"}},{"cell_type":"markdown","source":"### Visualizing Sample Images with Bounding Boxes, True Labels, and Image Dimensions","metadata":{"id":"OjkvTLV-gUfP"}},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Define the paths to the images and labels directories\ntrain_images = \"/kaggle/input/cardetection/car/train/images\"\ntrain_labels = \"/kaggle/input/cardetection/car/train/labels\"\n\n# Get a list of all the image files in the training images directory\nimage_files = os.listdir(train_images)\n\n# Choose 6 random image files from the list\nrandom_images = random.sample(image_files, 6)\n\n# Set up the plot\nfig, axs = plt.subplots(2, 3, figsize=(12, 8))\n\n# Load class names\nclass_names = {\n    0: 'Green Light',\n    1: 'Red Light',\n    2: 'Speed Limit 10',\n    3: 'Speed Limit 100',\n    4: 'Speed Limit 110',\n    5: 'Speed Limit 120',\n    6: 'Speed Limit 20',\n    7: 'Speed Limit 30',\n    8: 'Speed Limit 40',\n    9: 'Speed Limit 50',\n    10:'Speed Limit 60',\n    11:'Speed Limit 70',\n    12:'Speed Limit 80',\n    13:'Speed Limit 90',\n    14:'Stop'\n}\n\nfor i, image_file in enumerate(random_images):\n    row = i // 3\n    col = i % 3\n\n    # Load the image\n    image_path = os.path.join(train_images, image_file)\n    image = cv2.imread(image_path)\n\n    # Load the labels for this image\n    label_file = os.path.splitext(image_file)[0] + \".txt\"\n    label_path = os.path.join(train_labels, label_file)\n    with open(label_path, \"r\") as f:\n        labels = f.read().strip().split(\"\\n\")\n\n    # Loop over the labels and plot the object detections\n    for label in labels:\n        if len(label.split()) != 5:\n            continue\n        class_id, x_center, y_center, width, height = map(float, label.split())\n        x_min = int((x_center - width/2) * image.shape[1])\n        y_min = int((y_center - height/2) * image.shape[0])\n        x_max = int((x_center + width/2) * image.shape[1])\n        y_max = int((y_center + height/2) * image.shape[0])\n        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 5)\n\n        # Add class name to the bounding box\n        class_name = class_names.get(int(class_id), \"Unknown\")\n        cv2.putText(image, class_name, (x_min, y_min - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n\n    # Display the image with bounding boxes\n    axs[row, col].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    axs[row, col].axis('off')\n\n    # Display the image dimensions and channels\n    h, w, c = image.shape\n    axs[row, col].set_title(f\"{w}x{h}, {c} channels\")\n\nplt.show()","metadata":{"id":"RA5Sc-VN2CCs","outputId":"91e5d625-7f03-4c7e-eb8a-61d410dbac69","execution":{"iopub.status.busy":"2024-07-18T18:33:51.847366Z","iopub.execute_input":"2024-07-18T18:33:51.847706Z","iopub.status.idle":"2024-07-18T18:33:53.160087Z","shell.execute_reply.started":"2024-07-18T18:33:51.847674Z","shell.execute_reply":"2024-07-18T18:33:53.159108Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"preparation\"></a>\n# <p style=\"background-image: url(https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/22606/images/61ae8d7-6831-7f5c-8b52-01d30ba74ffc_og-ultralytics.jpeg);font-family:tahoma; font-weight:bold; color:navy; font-size:100%;text-align:center; border-radius:50px 50px;padding:7px; border:solid 2px #09375b; box-shadow: 10px 10px 10px #042b4c\"> 5. Data Preparation</p>\n‚úÖ [Tabel of Contents](#contents_tabel)","metadata":{"id":"LnwxP-a3gUfR"}},{"cell_type":"markdown","source":"<div style=\"background-color:#a6e7ff; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h1 style=\"font-size: 24px; font-family: calibri; color: #141140;\"><b>YOLOv8 Dataset Structure üöÄ</b></h1>\n    <p style=\"font-size: 20px; font-family: calibri; line-height: 1.5em; text-indent: 20px;\">\n        The dataset structure for YOLOv8 is well-organized, so I do not change it.\n    </p>\n</div>\n\n ```\n‚îú‚îÄ‚îÄ data.yaml\n‚îú‚îÄ‚îÄ test\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ images\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ labels\n‚îú‚îÄ‚îÄ train\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ images\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ labels\n‚îî‚îÄ‚îÄ valid\n    ‚îú‚îÄ‚îÄ images\n    ‚îî‚îÄ‚îÄ labels\n```","metadata":{"id":"fqdVlmNxgUfR"}},{"cell_type":"markdown","source":"<div style=\"background-color:#a6e7ff; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); line-height: 2em; \">\n    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>However, I recommend copying the dataset to: /kaggle/working/dataset.</b></h2>\n    <p style=\"font-size:18px; font-family:calibri; line-height: 2em; text-indent: 20px;\">\n        Now, let‚Äôs explore the reasons behind my recommendation:\n    </p>\n    <ul style=\"font-size:18px; font-family:calibri; line-height: 2em;\">\n        <ul> <code style=\"background-color:gold; font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\"> 1./kaggle/input/road-sign-recognition</code> </ul>\n        <li>This path refers to the input directory in Kaggle.</li>\n        <li>It contains read-only data that Kaggle provides for your project.</li>\n        <li>You can access datasets, pre-trained models, and other resources from this directory. However, you cannot modify or write to this directory during your code execution.</li>\n        <li>It‚Äôs meant for loading data into your notebook, not for saving new files.</li>\n        <li>You can access datasets, pre-trained models, and other resources from this directory.</li>\n        <ul> <code style=\"background-color:gold\"> 2. /kaggle/working/dataset</code> </ul>\n        <li>This path refers to the working directory in Kaggle.</li>\n        <li>It is writable, meaning you can create, modify, and save files here.</li>\n        <li>You have full control over this directory during your code execution.</li>\n        <li>It‚Äôs where you can organize your custom datasets, intermediate files, and model checkpoints.</li>\n        <li>When you save files here, they persist across different code cells and sessions.</li>\n    </ul>\n</div>","metadata":{"id":"w4LzANfd2CCt"}},{"cell_type":"markdown","source":"<div style=\"background-color:#a6e7ff  ; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b> üîç Exploring the data.yaml File</b></h1>\n    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n        The <strong>data.yaml</strong> file plays a crucial role in configuring our model. It specifies the locations of training and validation images and defines the target class, which in this case is labeled as <strong>Green Light</strong>, <strong>Red Light</strong>, <strong>Speed Limit 10</strong>, <strong>Speed Limit 100</strong>, <strong>Speed Limit 110</strong>, <strong>Speed Limit 120</strong>, <strong>Speed Limit 20</strong>, <strong>Speed Limit 30</strong>, <strong>Speed Limit 40</strong>, <strong>Speed Limit 50</strong>, <strong>Speed Limit 60</strong>, <strong>Speed Limit 70</strong>, <strong>Speed Limit 80</strong>, <strong>Speed Limit 90</strong> and <strong>STOP</strong>. This file is essential for ensuring that our Ultralytics YOLOv8 model effectively learns from our specific dataset, as it precisely instructs the model on what to identify and where to focus its attention.\n    </p>\n</div>","metadata":{"id":"Zg15DbDBgUfS"}},{"cell_type":"code","source":"from glob import glob\nimport os\nimport shutil\n\ndef make_directory(source_folder, destination_folder):\n    os.makedirs(destination_folder, exist_ok=True)\n    files = glob(f\"{source_folder}/*\")\n    for file in files:\n        if os.path.isdir(file):\n            shutil.copytree(file, os.path.join(destination_folder, os.path.basename(file)))\n        elif os.path.isfile(file):\n            shutil.copy(file, destination_folder)\n\nsource_folder = \"/kaggle/input/cardetection\"\ndestination_folder = '/kaggle/working/cardetection'\n\nmake_directory(source_folder, destination_folder)\nprint(\"Folder structure created successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-07-18T18:33:53.161316Z","iopub.execute_input":"2024-07-18T18:33:53.161614Z","iopub.status.idle":"2024-07-18T18:34:36.994726Z","shell.execute_reply.started":"2024-07-18T18:33:53.161589Z","shell.execute_reply":"2024-07-18T18:34:36.993755Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"training\"></a>\n# <p style=\"background-image: url(https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/22606/images/61ae8d7-6831-7f5c-8b52-01d30ba74ffc_og-ultralytics.jpeg);font-family:tahoma; font-weight:bold; color:navy; font-size:100%;text-align:center; border-radius:50px 50px;padding:7px; border:solid 2px #09375b; box-shadow: 10px 10px 10px #042b4c\"> 6. üöÄ Model Training</p>\n‚úÖ [Tabel of Contents](#contents_tabel)","metadata":{"id":"SH9NMPVUgUfZ"}},{"cell_type":"code","source":"# Use a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n  \n\n# Train the model with your custom file\nresults = model.train(\n    data = '/kaggle/working/cardetection/car/data.yaml',\n    epochs = 100,\n    imgsz=416,\n    device=0,\n    patience=50,\n    batch = 32,\n    optimizer='auto',\n    lr0 = 0.0001,\n    lrf=0.1,\n    seed=0,\n    dropout=0.2,\n    plots=True)","metadata":{"id":"GoT6ojdU2CCt","outputId":"c265fe5f-566c-4502-9e26-89d6b80c824d","execution":{"iopub.status.busy":"2024-07-18T18:39:11.841947Z","iopub.execute_input":"2024-07-18T18:39:11.842312Z","iopub.status.idle":"2024-07-18T19:19:11.73186Z","shell.execute_reply.started":"2024-07-18T18:39:11.842283Z","shell.execute_reply":"2024-07-18T19:19:11.730944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model\nmodel.save('yolov8_trained.pt')\nprint(\"Model trained and saved successfully!\")","metadata":{"id":"wkm0Uqdg2CCu","outputId":"1a7cf5d0-2a9e-4b02-d2e0-12cd70632ac6","execution":{"iopub.status.busy":"2024-07-18T19:19:31.1088Z","iopub.execute_input":"2024-07-18T19:19:31.109704Z","iopub.status.idle":"2024-07-18T19:19:31.381111Z","shell.execute_reply.started":"2024-07-18T19:19:31.109672Z","shell.execute_reply":"2024-07-18T19:19:31.380186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"evaluation\"></a>\n# <p style=\"background-image: url(https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/22606/images/61ae8d7-6831-7f5c-8b52-01d30ba74ffc_og-ultralytics.jpeg);font-family:tahoma; font-weight:bold; color:navy; font-size:100%;text-align:center; border-radius:50px 50px;padding:7px; border:solid 2px #09375b; box-shadow: 10px 10px 10px #042b4c\"> 7. üßÆ Model Performance Evaluation</p>\n‚úÖ [Tabel of Contents](#contents_tabel)","metadata":{"id":"gwPZXylsVlYI"}},{"cell_type":"markdown","source":"<span style=\"color:#294B8E;font-weight:700;font-size:25px\">\n     üìÇ Listing Files in the Post-Training Directory</span>","metadata":{"id":"gRqUqI6PRnym"}},{"cell_type":"code","source":"# Define the path to the directory\npost_training_files_path ='/kaggle/working/runs/detect/train'\n\n# List the files in the directory\n!ls {post_training_files_path}","metadata":{"id":"YBmqqdVzRnym","execution":{"iopub.status.busy":"2024-07-18T19:19:44.527338Z","iopub.execute_input":"2024-07-18T19:19:44.527705Z","iopub.status.idle":"2024-07-18T19:19:45.567984Z","shell.execute_reply.started":"2024-07-18T19:19:44.527679Z","shell.execute_reply":"2024-07-18T19:19:45.566956Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:#294B8E;font-weight:700;font-size:25px\">\n     üëÄ Displaying Images from Post-Training Directory</span>","metadata":{"id":"xdjeFmSLVlYP"}},{"cell_type":"code","source":"from PIL import Image\nimport os\nimport matplotlib.pyplot as plt\n\n# Define the path to the directory containing the images\nimage_directory = \"/kaggle/working/runs/detect/train\"\n\n# Iterate through all image files in the directory\nfor filename in os.listdir(image_directory):\n    if filename.lower().endswith((\".jpg\",\".png\")):\n        image_path = os.path.join(image_directory, filename)\n        image = Image.open(image_path)\n\n        # Display the image\n        plt.figure(figsize=(12, 12), dpi=150)\n        plt.imshow(image)\n        plt.title(f\"Image: {filename}\", fontsize=20, fontweight='bold', color='blue')  # Customize font properties\n        plt.axis(\"off\")  # Hide axes\n        plt.show()","metadata":{"id":"Xe3VsNczRnym","execution":{"iopub.status.busy":"2024-07-18T19:35:02.654027Z","iopub.execute_input":"2024-07-18T19:35:02.654416Z","iopub.status.idle":"2024-07-18T19:35:29.729461Z","shell.execute_reply.started":"2024-07-18T19:35:02.654389Z","shell.execute_reply":"2024-07-18T19:35:29.727523Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:#294B8E;font-weight:700;font-size:25px\">\n    &#10148; Model Evaluation</span>","metadata":{"id":"8VJKQn542CCv"}},{"cell_type":"code","source":"weights_path = '/kaggle/working/runs/detect/train/weights'\n\n!ls {weights_path}","metadata":{"id":"ELlP0AmhRnyn","execution":{"iopub.status.busy":"2024-07-18T19:19:54.605972Z","iopub.execute_input":"2024-07-18T19:19:54.606884Z","iopub.status.idle":"2024-07-18T19:19:55.654892Z","shell.execute_reply.started":"2024-07-18T19:19:54.60685Z","shell.execute_reply":"2024-07-18T19:19:55.653719Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:#294B8E;font-weight:700;font-size:20px\">\n    &#10148;&#10148; Evaluate Model Performance on the Validation Set</span>","metadata":{"id":"y2pgWJL3VlYT"}},{"cell_type":"markdown","source":"    We set the path to the best model weights and load them into the YOLO model for evaluation.","metadata":{"id":"uLTL3H2z2CCw"}},{"cell_type":"code","source":"# Construct the path to the best model weights file using os.path.join\nbest_model_path = os.path.join(weights_path, 'best.pt')\n\n# Load the best model weights into the YOLO model\nbest_model = YOLO(best_model_path)\n\n# Validate the best model using the validation set with default parameters\nmetrics = best_model.val(split='val')","metadata":{"id":"L7Kh1da0Rnyp","execution":{"iopub.status.busy":"2024-07-18T19:20:09.516602Z","iopub.execute_input":"2024-07-18T19:20:09.517395Z","iopub.status.idle":"2024-07-18T19:20:20.42409Z","shell.execute_reply.started":"2024-07-18T19:20:09.517357Z","shell.execute_reply":"2024-07-18T19:20:20.422977Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The validation data is located in the following directory:","metadata":{"id":"O5JCaVn12CCx"}},{"cell_type":"code","source":"val_path = '/kaggle/working/runs/detect/val'\n\n!ls {val_path}","metadata":{"id":"bZqrwwg_Rnyp","execution":{"iopub.status.busy":"2024-07-18T19:20:25.870972Z","iopub.execute_input":"2024-07-18T19:20:25.871759Z","iopub.status.idle":"2024-07-18T19:20:26.919418Z","shell.execute_reply.started":"2024-07-18T19:20:25.871717Z","shell.execute_reply":"2024-07-18T19:20:26.918255Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:#294B8E;font-weight:700;font-size:20px\">\n    &#10148;&#10148; Quantifying and Visualizing Metrics</span>","metadata":{"id":"2gnGKzyB2CCy"}},{"cell_type":"markdown","source":"Finally, we compute and display the evaluation metrics:\n","metadata":{"id":"3mxJty1h2CCy"}},{"cell_type":"code","source":"# Final results of test data\nprecision = round(metrics.results_dict[\"metrics/precision(B)\"] * 100, 2)\nrecall = round(metrics.results_dict[\"metrics/recall(B)\"] * 100, 2)\nmAP50 = round(metrics.results_dict[\"metrics/mAP50(B)\"] * 100, 2)\nmAP50_95 = round(metrics.results_dict[\"metrics/mAP50-95(B)\"] * 100, 2)\n\n# Create a list of metric names and corresponding values\nmetrics_table = [\n    [\"Precision\", precision],\n    [\"Recall\", recall],\n    [\"mAP50\", mAP50],\n    [\"mAP50-95\", mAP50_95]\n]\n\n# Print the table with headers\nprint(\"\\033[1mMetrics of Test Data:\\033[0m\")\nfor metric, value in metrics_table:\n    print(f\"{metric:<10}: {value:.2f}%\")","metadata":{"id":"Yu3MXSYhRnyq","outputId":"e2502c0b-d376-4d15-b926-875212b0cca4","execution":{"iopub.status.busy":"2024-07-18T19:20:33.691058Z","iopub.execute_input":"2024-07-18T19:20:33.691515Z","iopub.status.idle":"2024-07-18T19:20:33.701942Z","shell.execute_reply.started":"2024-07-18T19:20:33.691481Z","shell.execute_reply":"2024-07-18T19:20:33.700836Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n%matplotlib inline\n# Create the barplot\nax = sns.barplot(x=['mAP50-95', 'mAP50', 'Recall', 'Precision'], y=[mAP50_95, mAP50, recall, precision])\n\n# Set the title and axis labels\nax.set_title('YOLO Evaluation Metrics')\nax.set_xlabel('Metric')\nax.set_ylabel('Value')\n\n# Set the figure size\nfig = plt.gcf()\nfig.set_size_inches(8, 6)\n\n# Add the values on top of the bars\nfor p in ax.patches:\n    ax.annotate('{:.2f}'.format(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n\n# Show the plot\nplt.show()","metadata":{"id":"BOMoRxwyRnyq","outputId":"96e60d84-4582-41ff-fd7c-6291ca972370","execution":{"iopub.status.busy":"2024-07-18T19:20:41.108587Z","iopub.execute_input":"2024-07-18T19:20:41.109298Z","iopub.status.idle":"2024-07-18T19:20:41.437926Z","shell.execute_reply.started":"2024-07-18T19:20:41.109263Z","shell.execute_reply":"2024-07-18T19:20:41.43684Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"inference\"></a>\n# <p style=\"background-image: url(https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/22606/images/61ae8d7-6831-7f5c-8b52-01d30ba74ffc_og-ultralytics.jpeg);font-family:tahoma; font-weight:bold; color:navy; font-size:100%;text-align:center; border-radius:50px 50px;padding:7px; border:solid 2px #09375b; box-shadow: 10px 10px 10px #042b4c\"> 8. Model Inference</p>\n‚úÖ [Tabel of Contents](#contents_tabel)","metadata":{"id":"aojYv-HuVlYZ"}},{"cell_type":"markdown","source":"<span style=\"color:navy;font-weight:700;font-size:25px\">\n    &#10148; Model Inference on Test Images Set</span>","metadata":{"id":"XHCJ3pF9VlYk"}},{"cell_type":"code","source":"import os\n\n# Specify the directory containing the images\nimage_dir = \"/kaggle/input/cardetection/car/test/images\"\n\n# Get a list of image files\nimage_files = [file for file in os.listdir(image_dir) if file.lower().endswith((\".jpg\"))]\n\n# Loop over the first 15 image files\nfor image_file in image_files[:15]:\n    image_path = os.path.join(image_dir, image_file)\n    results = best_model.predict(image_path, imgsz=416, save=True, conf=0.6)","metadata":{"id":"PWGLADat2CCz","execution":{"iopub.status.busy":"2024-07-18T19:21:16.022731Z","iopub.execute_input":"2024-07-18T19:21:16.023122Z","iopub.status.idle":"2024-07-18T19:21:16.346112Z","shell.execute_reply.started":"2024-07-18T19:21:16.023094Z","shell.execute_reply":"2024-07-18T19:21:16.345267Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"folder being saved: \",results[0].save_dir)","metadata":{"id":"WyexF0S9VlYn","execution":{"iopub.status.busy":"2024-07-18T19:21:27.876594Z","iopub.execute_input":"2024-07-18T19:21:27.877078Z","iopub.status.idle":"2024-07-18T19:21:27.882431Z","shell.execute_reply.started":"2024-07-18T19:21:27.877045Z","shell.execute_reply":"2024-07-18T19:21:27.881353Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from glob import glob\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n\n# Get the list of image paths\nimage_paths = glob('/kaggle/working/runs/detect/predict/*.jpg')[6:12]  # Adjust to get up to 9 images\n\n# Set up the plot with 2 rows and 3 columns\nfig, axs = plt.subplots(2, 3, figsize=(15, 10))\n\n# Loop through the image paths and display them\nfor i, image_path in enumerate(image_paths):\n    row = i // 3\n    col = i % 3\n    image = Image.open(image_path)\n    axs[row, col].imshow(image)\n    axs[row, col].axis('off')\n\n# Hide any unused subplots\nfor j in range(len(image_paths), 6):\n    row = j // 3\n    col = j % 3\n    axs[row, col].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"xl0L5oj72CCz","execution":{"iopub.status.busy":"2024-07-18T19:21:34.563873Z","iopub.execute_input":"2024-07-18T19:21:34.564492Z","iopub.status.idle":"2024-07-18T19:21:35.751405Z","shell.execute_reply.started":"2024-07-18T19:21:34.56446Z","shell.execute_reply":"2024-07-18T19:21:35.749298Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:navy;font-weight:700;font-size:25px\">\n    &#10148; Model Inference on an Unseen Test Image</span>","metadata":{"id":"S2bIcHD-2CC0"}},{"cell_type":"code","source":"!yolo task=detect mode=predict model=/kaggle/working/runs/detect/train/weights/best.pt source='/kaggle/input/road-sign'","metadata":{"id":"9xl8eP6h2CC0","execution":{"iopub.status.busy":"2024-07-18T19:22:55.80976Z","iopub.execute_input":"2024-07-18T19:22:55.810158Z","iopub.status.idle":"2024-07-18T19:23:05.468345Z","shell.execute_reply.started":"2024-07-18T19:22:55.810126Z","shell.execute_reply":"2024-07-18T19:23:05.467198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_path = '/kaggle/working/runs/detect/predict2/Test Image.jpg'\nimg = plt.imread(img_path)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(img)\nplt.axis(False)\nplt.show()","metadata":{"id":"NvnHzuKN2CC0","execution":{"iopub.status.busy":"2024-07-18T19:23:11.498426Z","iopub.execute_input":"2024-07-18T19:23:11.498836Z","iopub.status.idle":"2024-07-18T19:23:11.929125Z","shell.execute_reply.started":"2024-07-18T19:23:11.498804Z","shell.execute_reply":"2024-07-18T19:23:11.927955Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:navy;font-weight:700;font-size:25px\">\n    &#10148; Model Inference on an Unseen Test Video</span>","metadata":{"id":"BWFC2EOA2CC0"}},{"cell_type":"code","source":"# Define the path to the example video stored online\nexample_video_path = '/kaggle/working/cardetection/video.mp4'\n\n# Use the best_model to predict objects in the video, save the output, and enable streaming\nbest_model.predict(source=example_video_path, conf=0.60, save=True)","metadata":{"id":"V791KFypVlYr","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-18T19:23:24.559489Z","iopub.execute_input":"2024-07-18T19:23:24.5603Z","iopub.status.idle":"2024-07-18T19:23:31.76453Z","shell.execute_reply.started":"2024-07-18T19:23:24.560269Z","shell.execute_reply":"2024-07-18T19:23:31.763416Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import the Video class from IPython.display to embed videos within the Jupyter notebook\nfrom IPython.display import Video","metadata":{"id":"xq5lOBH_VlYs","execution":{"iopub.status.busy":"2024-07-18T19:23:56.176885Z","iopub.execute_input":"2024-07-18T19:23:56.177547Z","iopub.status.idle":"2024-07-18T19:23:56.181677Z","shell.execute_reply.started":"2024-07-18T19:23:56.177513Z","shell.execute_reply":"2024-07-18T19:23:56.180674Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the resulting video from avi to mp4 file format\n!ffmpeg -y -loglevel panic -i /kaggle/working/runs/detect/predict/video.avi result_out.mp4\n\n#Display the converted MP4 video within the notebook with specified width\nVideo(\"result_out.mp4\",embed=True, width=960)","metadata":{"id":"6JFhXaNV2CC1","execution":{"iopub.status.busy":"2024-07-18T19:24:05.359809Z","iopub.execute_input":"2024-07-18T19:24:05.360519Z","iopub.status.idle":"2024-07-18T19:24:09.723896Z","shell.execute_reply.started":"2024-07-18T19:24:05.360486Z","shell.execute_reply":"2024-07-18T19:24:09.722328Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"export\"></a>\n# <p style=\"background-image: url(https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/22606/images/61ae8d7-6831-7f5c-8b52-01d30ba74ffc_og-ultralytics.jpeg);font-family:tahoma; font-weight:bold; color:navy; font-size:100%;text-align:center; border-radius:50px 50px;padding:7px; border:solid 2px #09375b; box-shadow: 10px 10px 10px #042b4c\"> 9. Export the Model to ONNX Format</p>\n‚úÖ [Tabel of Contents](#contents_tabel)","metadata":{"id":"S_O_Pi01VlYt"}},{"cell_type":"markdown","source":"When training a model, the ultimate objective is to deploy it for real-world applications. In the context of Ultralytics YOLOv8, the export model provides a powerful set of options for converting your trained model into various formats. This versatility ensures that your model can be seamlessly deployed across different platforms and devices.\n\nRemember, the export process is crucial for taking your model from the training environment to practical use, so pay attention to the details and explore the available options to optimize its deployment. üöÄ","metadata":{"id":"OYrz4gJs2CC1"}},{"cell_type":"code","source":"# Export the model to ONNX (Open Neural Network Exchange) format\nonnx_path = best_model.export(format=\"onnx\")","metadata":{"id":"lY0Jt3okVlYu","execution":{"iopub.status.busy":"2024-07-18T19:24:25.506634Z","iopub.execute_input":"2024-07-18T19:24:25.50761Z","iopub.status.idle":"2024-07-18T19:24:27.651954Z","shell.execute_reply.started":"2024-07-18T19:24:25.507575Z","shell.execute_reply":"2024-07-18T19:24:27.650928Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feel free to leave a comment if you liked my work üòä","metadata":{"id":"oLJnJmib2CC1"}}]}