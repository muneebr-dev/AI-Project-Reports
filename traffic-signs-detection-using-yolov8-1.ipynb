{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":930.231796,"end_time":"2022-12-25T15:05:01.303118","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-12-25T14:49:31.071322","version":"2.3.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"01bc8c18b6344986b91b372202add723":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc246ab517c347f0af25a19f80aa0304","max":57458,"min":0,"orientation":"horizontal","style":"IPY_MODEL_180163535ef64b2eb54ea7ecaccad327","value":57458}},"180163535ef64b2eb54ea7ecaccad327":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1812cc6e648049ffb8e8a968c0beb21a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18989c618af048dcadae60133997ba6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d541dcbf54bf4b6c9e0de8feab8843bd","placeholder":"​","style":"IPY_MODEL_63dc732c12954e25bbd465d97e3c258b","value":"100%"}},"1aa2398c82da45439b729c63591b41f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c51a9fe274945529ba17d7591f583ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18989c618af048dcadae60133997ba6a","IPY_MODEL_97aeea9b0bbf4405857af58f4a92eaa2","IPY_MODEL_7a34d15617b544f389eaf04f95c32e9b"],"layout":"IPY_MODEL_2e338e1e9eb847b0a07ab949e56c55ec"}},"2e338e1e9eb847b0a07ab949e56c55ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39fa2448024f43f7b76a4c0c06636568":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54729a8a899f47e4a6bfd71a7a43cf7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39fa2448024f43f7b76a4c0c06636568","placeholder":"​","style":"IPY_MODEL_1aa2398c82da45439b729c63591b41f7","value":" 57458/57458 [08:38&lt;00:00, 97.95it/s]"}},"63dc732c12954e25bbd465d97e3c258b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c9ccb4155e54220aa61f3e9449bd113":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7e83309bfd04350ba8e7344b68d8968","IPY_MODEL_01bc8c18b6344986b91b372202add723","IPY_MODEL_54729a8a899f47e4a6bfd71a7a43cf7c"],"layout":"IPY_MODEL_78297aca5b0b43ac990e4eaaa3163967"}},"78297aca5b0b43ac990e4eaaa3163967":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a34d15617b544f389eaf04f95c32e9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb7558f5a0f441b4a339e1836134d833","placeholder":"​","style":"IPY_MODEL_1812cc6e648049ffb8e8a968c0beb21a","value":" 50/50 [05:39&lt;00:00,  6.34s/it]"}},"97aeea9b0bbf4405857af58f4a92eaa2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ade48cb84426474f903dc0006ad07778","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df0d061b7fbe4e1ead3a24e1e0c12a15","value":50}},"9ced464273c44b6ab7d7ab8e81413bff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ade48cb84426474f903dc0006ad07778":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2679da6c9f2455fbc754b2e27f5739e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc246ab517c347f0af25a19f80aa0304":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d541dcbf54bf4b6c9e0de8feab8843bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7e83309bfd04350ba8e7344b68d8968":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ced464273c44b6ab7d7ab8e81413bff","placeholder":"​","style":"IPY_MODEL_c2679da6c9f2455fbc754b2e27f5739e","value":"100%"}},"df0d061b7fbe4e1ead3a24e1e0c12a15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb7558f5a0f441b4a339e1836134d833":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8968421,"sourceType":"datasetVersion","datasetId":4066836}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <b> <span style='color:#e61227'>|</span> Traffic Signs Detection Using YOLOv8 </b> ","metadata":{}},{"cell_type":"markdown","source":"![](https://learnopencv.com/wp-content/uploads/2023/01/evolution-of-yolo-models-1024x576.png)","metadata":{}},{"cell_type":"markdown","source":"## <b>1 <span style='color:#e61227'>|</span> Introduction</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#03112A;font-size:150%;\n            letter-spacing:1.0px;background-image: url(https://i.imgur.com/GVd0La1.png)\">\n    <p style=\"padding: 8px;color:white;\"><b><b><span style='color:#e61227''>1.1 |</span></b> Object Detection </b></p>\n</div>\n\n**Computer vision** is a field of artificial intelligence that focuses on teaching computers to interpret and understand visual information. One popular and powerful technique used in computer vision for object detection is called **YOLO**, which stands for \"You Only Look Once\".\n\nYOLO aims to identify and locate objects in an image or video stream in real-time. Unlike traditional methods that rely on complex pipelines and multiple passes, YOLO takes a different approach by treating object detection as a single regression problem.\n\nThis algorithm divides the input image into a grid and predicts bounding boxes and class probabilities for objects within each grid cell. It simultaneously predicts the class labels and their corresponding bounding boxes, making it incredibly efficient and fast. YOLO is known for its real-time performance, enabling it to process images and videos at impressive speeds.\n\nBy leveraging **deep convolutional neural networks**, YOLO can learn to recognize a wide range of objects and accurately localize them within an image. It can detect multiple objects of different classes simultaneously, making it particularly useful for applications where **real-time processing** and high detection accuracy are crucial, such as **autonomous driving**, **video surveillance**, and **robotics**.\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#03112A;font-size:150%;\n            letter-spacing:1.0px;background-image: url(https://i.imgur.com/GVd0La1.png)\">\n    <p style=\"padding: 8px;color:white;\"><b><b><span style='color:#e61227''>1.2 |</span></b> YOLOv8 </b></p>\n</div>\n\n**YOLOv8** is the latest version of the YOLO AI model developed by **Ultralytics**, which has shown effectiveness in tackling tasks such as **classification**, **object detection**, and **image segmentation**. **YOLOv8 models** are fast, accurate, and easy to use, making them ideal for various object detection and image segmentation tasks. They can be trained on large datasets and run on diverse hardware platforms, from **CPUs** to **GPUs**.\nYOLOv8 _detection_ models have no suffix and are the default YOLOv8 models, i.e. **yolov8n.pt** ,and are pre-trained on COCO. See [Detection Docs ](https://docs.ultralytics.com/tasks/detect/) for full details.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#03112A;font-size:150%;\n            letter-spacing:1.0px;background-image: url(https://i.imgur.com/GVd0La1.png)\">\n    <p style=\"padding: 8px;color:white;\"><b><b><span style='color:#e61227'>1.3 |</span></b> Study Aim </b></p>\n</div>\n\nImplementing **Sign Detection** using **YOLOv8** holds tremendous potential across a wide range of practical applications. For example, it can greatly enhance **traffic management systems**, allowing for the efficient detection and recognition of various traffic signs. This technology can play a vital role in improving road safety by enabling vehicles to accurately interpret and respond to the information conveyed by road signs. Moreover, sign detection using YOLOv8 can assist in urban planning and infrastructure development by analyzing the presence and condition of signs in different areas\n\nBy undertaking this project, I aimed to harness the power of YOLOv8 to develop a **reliable signs detection solution** with the potential to improve various domains that rely on precise and efficient signs identification.\n","metadata":{}},{"cell_type":"markdown","source":"## <b>2 <span style='color:#e61227'>|</span> Install And Import Essential Libreries</b> \n","metadata":{}},{"cell_type":"code","source":"# Install Essential Libraries\n!pip install ultralytics","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-07-16T13:49:07.446475Z","iopub.execute_input":"2024-07-16T13:49:07.446831Z","iopub.status.idle":"2024-07-16T13:49:21.404722Z","shell.execute_reply.started":"2024-07-16T13:49:07.446801Z","shell.execute_reply":"2024-07-16T13:49:21.403629Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import Essential Libraries\nimport os\nimport random\nimport pandas as pd\nfrom PIL import Image\nimport cv2\nfrom ultralytics import YOLO\nfrom IPython.display import Video\nimport numpy as np  \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='darkgrid')\nimport pathlib\nimport glob\nfrom tqdm.notebook import trange, tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-16T13:49:21.406456Z","iopub.execute_input":"2024-07-16T13:49:21.406738Z","iopub.status.idle":"2024-07-16T13:49:25.588672Z","shell.execute_reply.started":"2024-07-16T13:49:21.406707Z","shell.execute_reply":"2024-07-16T13:49:25.587754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure the visual appearance of Seaborn plots\nsns.set(rc={'axes.facecolor': '#eae8fa'}, style='darkgrid')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T13:49:25.589897Z","iopub.execute_input":"2024-07-16T13:49:25.590292Z","iopub.status.idle":"2024-07-16T13:49:25.595062Z","shell.execute_reply.started":"2024-07-16T13:49:25.590268Z","shell.execute_reply":"2024-07-16T13:49:25.594213Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <b>3 <span style='color:#e61227'>|</span> Dataset</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#03112A;font-size:150%;\n            letter-spacing:1.0px;background-image: url(https://i.imgur.com/GVd0La1.png)\">\n    <p style=\"padding: 8px;color:white;\"><b><b><span style='color:#e61227'>3.1 |</span></b> Show Original Images Before Detect </b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"***3.1.1. Show Some Images From TrainSet***","metadata":{}},{"cell_type":"code","source":"Image_dir = '/kaggle/input/cardetection/car/train/images'\n\nnum_samples = 9\nimage_files = os.listdir(Image_dir)\n\n# Randomly select num_samples images\nrand_images = random.sample(image_files, num_samples)\n\nfig, axes = plt.subplots(3, 3, figsize=(11, 11))\n\nfor i in range(num_samples):\n    image = rand_images[i]\n    ax = axes[i // 3, i % 3]\n    ax.imshow(plt.imread(os.path.join(Image_dir, image)))\n    ax.set_title(f'Image {i+1}')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T13:49:25.597046Z","iopub.execute_input":"2024-07-16T13:49:25.597304Z","iopub.status.idle":"2024-07-16T13:49:28.395251Z","shell.execute_reply.started":"2024-07-16T13:49:25.597281Z","shell.execute_reply":"2024-07-16T13:49:28.39402Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***3.1.2. Get Shape Of An Image For Using In Training Step***","metadata":{}},{"cell_type":"code","source":"# Get the size of the image\nimage = cv2.imread(\"/kaggle/input/cardetection/car/train/images/00000_00000_00012_png.rf.23f94508dba03ef2f8bd187da2ec9c26.jpg\")\nh, w, c = image.shape\nprint(f\"The image has dimensions {w}x{h} and {c} channels.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-16T13:49:28.396532Z","iopub.execute_input":"2024-07-16T13:49:28.396841Z","iopub.status.idle":"2024-07-16T13:49:28.423164Z","shell.execute_reply.started":"2024-07-16T13:49:28.396807Z","shell.execute_reply":"2024-07-16T13:49:28.422252Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <b>4 <span style='color:#e61227'>|</span> Try Pre-trained YOLOv8 For Detect Traffic Signs  </b>","metadata":{}},{"cell_type":"code","source":"# Use a pretrained YOLOv8n model\nmodel = YOLO(\"yolov8n.pt\") \n\n# Use the model to detect object\nimage = \"/kaggle/input/cardetection/car/train/images/FisheyeCamera_1_00228_png.rf.e7c43ee9b922f7b2327b8a00ccf46a4c.jpg\"\nresult_predict = model.predict(source = image, imgsz=(640))\n\n# show results\nplot = result_predict[0].plot()\nplot = cv2.cvtColor(plot, cv2.COLOR_BGR2RGB)\ndisplay(Image.fromarray(plot))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T13:49:28.424312Z","iopub.execute_input":"2024-07-16T13:49:28.424601Z","iopub.status.idle":"2024-07-16T13:49:30.605446Z","shell.execute_reply.started":"2024-07-16T13:49:28.424575Z","shell.execute_reply":"2024-07-16T13:49:30.604472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <b>5 <span style='color:#e61227'>|</span> YOLOv8-Based Traffic Signs Detection Model</b>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#03112A;font-size:150%;\n            letter-spacing:1.0px;background-image: url(https://i.imgur.com/GVd0La1.png)\">\n    <p style=\"padding: 8px;color:white;\"><b><b><span style='color:#e61227''>5.1 |</span></b> Model Training Using Customized Dataset Of Traffic Signs </b></p>\n</div>\n        \n***Mean Average Precision (mAP)*** is a metric used to evaluate the effectiveness of object detection algorithms in identifying and locating objects within images. It takes into account both precision and recall across different categories. By calculating the Average Precision (AP) for each category and taking the average, mAP provides an overall assessment of the algorithm's performance.\n\nIf the obtained mAP after the final epoch is not satisfactory, there are several options available to improve the results:\n- Extending the training process by increasing the number of **epochs**: Training for more epochs allows the model to learn more patterns and potentially improve its performance. You can specify a higher value for the --epochs argument when running the training command.\n- Experimenting with different **parameter values**: You can try adjusting various parameters to see if they have a positive impact on the results. Some parameters to consider are:\n- **Batch Size:** Changing the batch size can affect the convergence and generalization of the model. You can modify the --batch-size argument to find an optimal value.\n- **Initial Learning Rate (lr0)**: The initial learning rate determines the step size at the beginning of the training process. You can tune the --lr0 parameter to control how quickly the model learns.\n- **Learning Rate Range (lrf)**: The learning rate range determines the range of learning rates used during the training. Experimenting with different values for --lrf can help find a better learning rate schedule.\n- **Selecting a different optimizer**: The optimizer is responsible for updating the model's parameters based on the calculated gradients. Changing the optimizer can sometimes lead to better convergence and results. Ultralytics YOLOv8 supports different optimizers such as SGD, Adam, and RMSprop. You can try using a different optimizer by modifying the --optimizer argument.\n\nFor more details on resuming interrupted trainings and additional training options, you can refer to the Ultralytics YOLOv8 documentation.\n        \nIn order to optimize the performance of the model, we have conducted experiments with different parameter values and optimizers. During the training phase, I explored various combinations of epochs, batch sizes, initial learning rates (lr0), and dropout values. The following values were used for experimentation:\n\n- Epochs: ***10***, ***50***, ***100***\n- Batch sizes: ***8***, ***16***, ***32***, ***64***\n- Initial learning rates (lr0): ***0.001***, ***0.0003***, ***0.0001***\n- Dropout: ***0.15***, ***0.25***\n\nFurthermore, we have evaluated the model's performance using different optimizers, including **Adam**, **SGD**, and **auto**. By employing these optimizers, we aimed to assess their impact on the model's convergence and overall results.\n\nAfter rigorous experimentation and training, we are pleased to present the results of our findings. The performance of the YOLOv8 model was assessed based on various metrics, including Mean Average Precision (mAP). \n","metadata":{}},{"cell_type":"markdown","source":"***5.1.1. Training Step***","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade ultralytics ray\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T13:49:30.6066Z","iopub.execute_input":"2024-07-16T13:49:30.607352Z","iopub.status.idle":"2024-07-16T13:49:51.985914Z","shell.execute_reply.started":"2024-07-16T13:49:30.607318Z","shell.execute_reply":"2024-07-16T13:49:51.984755Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build from YAML and transfer weights\nFinal_model = YOLO('yolov8n.pt')  \n\n# Training The Final Model\nResult_Final_model = Final_model.train(data=\"/kaggle/input/cardetection/car/data.yaml\",epochs = 30, batch = -1, optimizer = 'auto')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T13:49:51.987524Z","iopub.execute_input":"2024-07-16T13:49:51.987903Z","iopub.status.idle":"2024-07-16T14:13:21.063821Z","shell.execute_reply.started":"2024-07-16T13:49:51.98787Z","shell.execute_reply":"2024-07-16T14:13:21.062748Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***5.1.2. Validation Step***","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef display_images(post_training_files_path, image_files):\n\n    for image_file in image_files:\n        image_path = os.path.join(post_training_files_path, image_file)\n        img = cv2.imread(image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        plt.figure(figsize=(10, 10), dpi=120)\n        plt.imshow(img)\n        plt.axis('off')\n        plt.show()\n\n# List of image files to display\nimage_files = [\n    'confusion_matrix_normalized.png',\n    'F1_curve.png',\n    'P_curve.png',\n    'R_curve.png',\n    'PR_curve.png',\n    'results.png'\n]\n\n# Path to the directory containing the images\npost_training_files_path = '/kaggle/working/runs/detect/train'\n\n# Display the images\ndisplay_images(post_training_files_path, image_files)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:13:21.065467Z","iopub.execute_input":"2024-07-16T14:13:21.065865Z","iopub.status.idle":"2024-07-16T14:13:25.857672Z","shell.execute_reply.started":"2024-07-16T14:13:21.065823Z","shell.execute_reply":"2024-07-16T14:13:25.856712Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Result_Final_model = pd.read_csv('/kaggle/working/runs/detect/train/results.csv')\nResult_Final_model.tail(10)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:13:25.861308Z","iopub.execute_input":"2024-07-16T14:13:25.861645Z","iopub.status.idle":"2024-07-16T14:13:25.893097Z","shell.execute_reply.started":"2024-07-16T14:13:25.861617Z","shell.execute_reply":"2024-07-16T14:13:25.892209Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read the results.csv file as a pandas dataframe\nResult_Final_model.columns = Result_Final_model.columns.str.strip()\n\n# Create subplots\nfig, axs = plt.subplots(nrows=5, ncols=2, figsize=(15, 15))\n\n# Plot the columns using seaborn\nsns.lineplot(x='epoch', y='train/box_loss', data=Result_Final_model, ax=axs[0,0])\nsns.lineplot(x='epoch', y='train/cls_loss', data=Result_Final_model, ax=axs[0,1])\nsns.lineplot(x='epoch', y='train/dfl_loss', data=Result_Final_model, ax=axs[1,0])\nsns.lineplot(x='epoch', y='metrics/precision(B)', data=Result_Final_model, ax=axs[1,1])\nsns.lineplot(x='epoch', y='metrics/recall(B)', data=Result_Final_model, ax=axs[2,0])\nsns.lineplot(x='epoch', y='metrics/mAP50(B)', data=Result_Final_model, ax=axs[2,1])\nsns.lineplot(x='epoch', y='metrics/mAP50-95(B)', data=Result_Final_model, ax=axs[3,0])\nsns.lineplot(x='epoch', y='val/box_loss', data=Result_Final_model, ax=axs[3,1])\nsns.lineplot(x='epoch', y='val/cls_loss', data=Result_Final_model, ax=axs[4,0])\nsns.lineplot(x='epoch', y='val/dfl_loss', data=Result_Final_model, ax=axs[4,1])\n\n# Set titles and axis labels for each subplot\naxs[0,0].set(title='Train Box Loss')\naxs[0,1].set(title='Train Class Loss')\naxs[1,0].set(title='Train DFL Loss')\naxs[1,1].set(title='Metrics Precision (B)')\naxs[2,0].set(title='Metrics Recall (B)')\naxs[2,1].set(title='Metrics mAP50 (B)')\naxs[3,0].set(title='Metrics mAP50-95 (B)')\naxs[3,1].set(title='Validation Box Loss')\naxs[4,0].set(title='Validation Class Loss')\naxs[4,1].set(title='Validation DFL Loss')\n\n\nplt.suptitle('Training Metrics and Loss', fontsize=24)\nplt.subplots_adjust(top=0.8)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:15:02.554756Z","iopub.execute_input":"2024-07-16T14:15:02.555162Z","iopub.status.idle":"2024-07-16T14:15:05.698329Z","shell.execute_reply.started":"2024-07-16T14:15:02.55513Z","shell.execute_reply":"2024-07-16T14:15:05.697384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#03112A;font-size:150%;\n            letter-spacing:1.0px;background-image: url(https://i.imgur.com/GVd0La1.png)\">\n    <p style=\"padding: 8px;color:white;\"><b><b><span style='color:#e61227''>5.2 |</span></b> Validation of the Model By TestSet </b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Loading the best performing model\nValid_model = YOLO('/kaggle/working/runs/detect/train/weights/best.pt')\n\n# Evaluating the model on the validset\nmetrics = Valid_model.val(split = 'val')\n\n# final results \nprint(\"precision(B): \", metrics.results_dict[\"metrics/precision(B)\"])\nprint(\"metrics/recall(B): \", metrics.results_dict[\"metrics/recall(B)\"])\nprint(\"metrics/mAP50(B): \", metrics.results_dict[\"metrics/mAP50(B)\"])\nprint(\"metrics/mAP50-95(B): \", metrics.results_dict[\"metrics/mAP50-95(B)\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:15:30.710801Z","iopub.execute_input":"2024-07-16T14:15:30.711196Z","iopub.status.idle":"2024-07-16T14:15:44.89101Z","shell.execute_reply.started":"2024-07-16T14:15:30.711162Z","shell.execute_reply":"2024-07-16T14:15:44.890016Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#03112A;font-size:150%;\n            letter-spacing:1.0px;background-image: url(https://i.imgur.com/GVd0La1.png)\">\n    <p style=\"padding: 8px;color:white;\"><b><b><span style='color:#e61227''>5.3 |</span></b> Making Predictions On Test Images </b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Normalization function\ndef normalize_image(image):\n    return image / 255.0\n\n# Image resizing function\ndef resize_image(image, size=(640, 640)):\n    return cv2.resize(image, size)\n\n# Path to validation images\ndataset_path = '/kaggle/input/cardetection/car'  # Place your dataset path here\nvalid_images_path = os.path.join(dataset_path, 'test', 'images')\n\n# List of all jpg images in the directory\nimage_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n\n# Check if there are images in the directory\nif len(image_files) > 0:\n    # Select 9 images at equal intervals\n    num_images = len(image_files)\n    step_size = max(1, num_images // 9)  # Ensure the interval is at least 1\n    selected_images = [image_files[i] for i in range(0, num_images, step_size)]\n\n    # Prepare subplots\n    fig, axes = plt.subplots(3, 3, figsize=(20, 21))\n    fig.suptitle('Validation Set Inferences', fontsize=24)\n\n    for i, ax in enumerate(axes.flatten()):\n        if i < len(selected_images):\n            image_path = os.path.join(valid_images_path, selected_images[i])\n            \n            # Load image\n            image = cv2.imread(image_path)\n            \n            # Check if the image is loaded correctly\n            if image is not None:\n                # Resize image\n                resized_image = resize_image(image, size=(640, 640))\n                # Normalize image\n                normalized_image = normalize_image(resized_image)\n                \n                # Convert the normalized image to uint8 data type\n                normalized_image_uint8 = (normalized_image * 255).astype(np.uint8)\n                \n                # Predict with the model\n                results = Valid_model.predict(source=normalized_image_uint8, imgsz=640, conf=0.5)\n                \n                # Plot image with labels\n                annotated_image = results[0].plot(line_width=1)\n                annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n                ax.imshow(annotated_image_rgb)\n            else:\n                print(f\"Failed to load image {image_path}\")\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:15:56.746669Z","iopub.execute_input":"2024-07-16T14:15:56.747714Z","iopub.status.idle":"2024-07-16T14:16:00.857727Z","shell.execute_reply.started":"2024-07-16T14:15:56.747663Z","shell.execute_reply":"2024-07-16T14:16:00.856349Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <b>6 <span style='color:#e61227'>|</span> Try Pre-trained YOLOv8 For Detect Traffic Signs From Video  </b>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#03112A;font-size:150%;\n            letter-spacing:1.0px;background-image: url(https://i.imgur.com/GVd0La1.png)\">\n    <p style=\"padding: 8px;color:white;\"><b><b><span style='color:#e61227'>6.1 |</span></b> Show Original Video Before Detect </b></p>\n</div>\n\n**Tip: Due to the substantial volume of the video generated from the image dataset, I have to utilize only a truncated segment of the original video for this project.**","metadata":{}},{"cell_type":"code","source":"# Convert mp4\n!ffmpeg -y -loglevel panic -i /kaggle/input/cardetection/video.mp4 output.mp4## <b>6 <span style='color:#e61227'>|</span> Export The Final Model Of Detect Traffic Signs </b>\n\n**Tip:** The ultimate goal of training a model is to deploy it for real-world applications. Export mode in Ultralytics YOLOv8 offers a versatile range of options for exporting your trained model to different formats, making it deployable across various platforms and devices.\n\n# Display the video\nVideo(\"output.mp4\", width=960)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:16:46.948575Z","iopub.execute_input":"2024-07-16T14:16:46.948928Z","iopub.status.idle":"2024-07-16T14:16:50.988449Z","shell.execute_reply.started":"2024-07-16T14:16:46.948901Z","shell.execute_reply":"2024-07-16T14:16:50.987281Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the model to detect signs\nValid_model.predict(source=\"/kaggle/input/cardetection/video.mp4\", show=True,save = True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show result\n# Convert format\n!ffmpeg -y -loglevel panic -i /kaggle/working/runs/detect/predict/video.avi result_out.mp4\n\n# Display the video \nVideo(\"result_out.mp4\", width=960)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-07-16T14:29:42.068924Z","iopub.execute_input":"2024-07-16T14:29:42.069321Z","iopub.status.idle":"2024-07-16T14:29:45.76812Z","shell.execute_reply.started":"2024-07-16T14:29:42.069287Z","shell.execute_reply":"2024-07-16T14:29:45.766968Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <b>7 <span style='color:#e61227'>|</span> Save Model  </b>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#03112A;font-size:150%;\n            letter-spacing:1.0px;background-image: url(https://i.imgur.com/GVd0La1.png)\">\n    <p style=\"padding: 8px;color:white;\"><b><b><span style='color:#e61227'></span></b> </b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Export the model\nValid_model.export(format='onnx')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:16:31.54728Z","iopub.execute_input":"2024-07-16T14:16:31.547925Z","iopub.status.idle":"2024-07-16T14:16:33.945698Z","shell.execute_reply.started":"2024-07-16T14:16:31.547895Z","shell.execute_reply":"2024-07-16T14:16:33.944769Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#03112A;font-size:150%;\n            letter-spacing:1.0px;background-image: url(https://i.imgur.com/GVd0La1.png)\">\n    <p style=\"padding: 8px;color:white;\"><b><b><span style='color:#e61227'></span></b> Thank you for taking the time to review my notebook! If you have any questions or criticisms, please kindly let me know in the comments section !!!!  </b></p>\n</div>\n","metadata":{}}]}